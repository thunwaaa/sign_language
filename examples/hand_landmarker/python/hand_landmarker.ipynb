{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thunwaaa/sign_language/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Hand Landmarks Detection with MediaPipe Tasks\n",
        "\n",
        "This notebook shows you how to use MediaPipe Tasks Python API to detect hand landmarks from images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with installing MediaPipe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf>=5.26.1,<6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyHeT5Wx0LNv",
        "outputId": "32a9441f-7d6a-4ba6-d7ff-fe52a1657a3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 6.0: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gxbHBsF-8Y_l",
        "outputId": "749878b5-a5d7-4d5c-d127-41e4a4b7f87d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe --upgrade"
      ],
      "metadata": {
        "id": "ud9tSu78qK-N",
        "outputId": "eeaa85c5-8247-4ef9-e12c-6575e2b5c64f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOIHSoTM0-f8",
        "outputId": "0d86a22a-3b8f-4d66-fd32-64d0017d0e66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.0 in /usr/local/lib/python3.11/dist-packages (1.26.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Then download an off-the-shelf model bundle. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker#models) for more information about this model bundle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYKAJ5nDU8-I"
      },
      "source": [
        "## Visualization utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s3E6NFV-00Qt"
      },
      "outputs": [],
      "source": [
        "#@markdown We implemented some functions to visualize the hand landmark detection results. <br/> Run the following cell to activate the functions.\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions\n",
        "\n",
        "# Setup MediaPipe solutions\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "# Constants\n",
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # vibrant green\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    hand_landmarks_list = detection_result.multi_hand_landmarks\n",
        "    handedness_list = detection_result.multi_handedness\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    # Loop through the detected hands to visualize.\n",
        "    for idx in range(len(hand_landmarks_list)):\n",
        "        hand_landmarks = hand_landmarks_list[idx]\n",
        "        handedness = handedness_list[idx]\n",
        "\n",
        "        # Draw the hand landmarks directly using MediaPipe's drawing utilities\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "        # Get the top left corner of the detected hand's bounding box.\n",
        "        height, width, _ = annotated_image.shape\n",
        "        x_coordinates = [landmark.x for landmark in hand_landmarks.landmark]\n",
        "        y_coordinates = [landmark.y for landmark in hand_landmarks.landmark]\n",
        "        text_x = int(min(x_coordinates) * width)\n",
        "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "        # Draw handedness (left or right hand) on the image.\n",
        "        cv2.putText(annotated_image, f\"{handedness.classification[0].label}\",\n",
        "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Download test image\n",
        "\n",
        "Let's grab a test image that we'll use later. The image is from [Unsplash](https://unsplash.com/photos/mt2fyrdXxzk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzXuqyIBlXer",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-skLwMBmMN_"
      },
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etBjSdwImQPw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "# อัปโหลดวิดีโอหลายๆ ไฟล์\n",
        "uploaded = files.upload()\n",
        "\n",
        "# แสดงรายชื่อไฟล์วิดีโอที่อัปโหลด\n",
        "print(\"ไฟล์วิดีโอที่อัปโหลด:\")\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"- {filename}\")\n",
        "\n",
        "# เก็บพาธของไฟล์วิดีโอทั้งหมด\n",
        "video_paths = list(uploaded.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "\n",
        "Here are the steps to run hand landmark detection using MediaPipe.\n",
        "\n",
        "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/python) to learn more about configuration options that this solution supports.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# import urllib.request\n",
        "\n",
        "# # MediaPipe initialization\n",
        "# BaseOptions = mp.tasks.BaseOptions\n",
        "# HandLandmarker = mp.tasks.vision.HandLandmarker\n",
        "# HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
        "# VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "# # Create a hand landmarker instance with the video mode\n",
        "# options = HandLandmarkerOptions(\n",
        "#     base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
        "#     running_mode=VisionRunningMode.VIDEO,\n",
        "#     num_hands=2)\n",
        "\n",
        "# # Open the video file\n",
        "# cap = cv2.VideoCapture(input_video_path)\n",
        "# if not cap.isOpened():\n",
        "#     print(f\"Error: Could not open video file {input_video_path}\")\n",
        "#     exit()\n",
        "\n",
        "# # Get video properties\n",
        "# width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "# total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# # Create video writer for output\n",
        "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "# out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# with HandLandmarker.create_from_options(options) as landmarker:\n",
        "#     # Initialize timestamp\n",
        "#     timestamp = 0\n",
        "#     frame_count = 0\n",
        "\n",
        "#     while cap.isOpened():\n",
        "#         success, frame = cap.read()\n",
        "#         if not success:\n",
        "#             print(\"End of video or error reading frame.\")\n",
        "#             break\n",
        "\n",
        "#         frame_count += 1\n",
        "#         # Optional: Print progress\n",
        "#         if frame_count % 10 == 0:\n",
        "#             print(f\"Processing frame {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
        "\n",
        "#         # Convert to RGB (MediaPipe requirement)\n",
        "#         frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "#         mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "\n",
        "#         # Process the frame\n",
        "#         results = landmarker.detect_for_video(mp_image, timestamp)\n",
        "#         timestamp += 1\n",
        "\n",
        "#         # Create a black canvas instead of using the original frame\n",
        "#         canvas = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "\n",
        "#         # Draw hand landmarks\n",
        "#         if results.hand_landmarks:\n",
        "#             for idx, hand_landmarks in enumerate(results.hand_landmarks):\n",
        "#                 # Get hand label (LEFT or RIGHT)\n",
        "#                 handedness = results.handedness[idx][0].category_name\n",
        "\n",
        "#                 # Get center of hand for text placement\n",
        "#                 x_values = [landmark.x for landmark in hand_landmarks]\n",
        "#                 y_values = [landmark.y for landmark in hand_landmarks]\n",
        "#                 center_x = int(sum(x_values) / len(x_values) * width)\n",
        "#                 center_y = int(sum(y_values) / len(y_values) * height)\n",
        "\n",
        "#                 # Draw connections with white color\n",
        "#                 for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
        "#                     start_idx = connection[0]\n",
        "#                     end_idx = connection[1]\n",
        "\n",
        "#                     start_point = (int(hand_landmarks[start_idx].x * width),\n",
        "#                                   int(hand_landmarks[start_idx].y * height))\n",
        "#                     end_point = (int(hand_landmarks[end_idx].x * width),\n",
        "#                                 int(hand_landmarks[end_idx].y * height))\n",
        "\n",
        "#                     cv2.line(canvas, start_point, end_point, (255, 255, 255), 2)  # White lines\n",
        "\n",
        "#                 # Draw landmarks with light blue color\n",
        "#                 for landmark in hand_landmarks:\n",
        "#                     landmark_point = (int(landmark.x * width),\n",
        "#                                      int(landmark.y * height))\n",
        "#                     cv2.circle(canvas, landmark_point, 5, (255, 200, 0), -1)  # Light blue dots\n",
        "\n",
        "#                 # Display hand label\n",
        "#                 color = (255, 100, 100) if handedness == \"LEFT\" else (100, 100, 255)  # Different colors for left/right\n",
        "#                 text_position = (center_x, center_y - 30)\n",
        "#                 cv2.putText(canvas, handedness, text_position,\n",
        "#                            cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "\n",
        "#         # Write the canvas to output video\n",
        "#         out.write(canvas)\n",
        "\n",
        "#         # Optional: Display the frame (comment out for faster processing)\n",
        "#         # cv2.imshow('Processing Video', canvas)\n",
        "#         # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         #     break\n",
        "\n",
        "# # Release resources\n",
        "# cap.release()\n",
        "# out.release()\n",
        "# cv2.destroyAllWindows()\n",
        "\n",
        "# print(f\"Processing complete. Output saved to {output_video_path}\")\n",
        "\n",
        "# files.download(output_video_path)"
      ],
      "metadata": {
        "id": "0HLgVzd8A3qT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_JVO3rvPD4RN",
        "outputId": "7c197481-89e0-4e40-9641-b640c81f79d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-05a193bd-7410-4049-9555-d91bac216e40\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-05a193bd-7410-4049-9555-d91bac216e40\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1.2.1.mp4 to 1.2.1 (4).mp4\n",
            "Saving output_hand_detection.mp4 to output_hand_detection (3).mp4\n",
            "กรุณาระบุชื่อท่าทาง (เช่น hello, thank_you, เป็นต้น): hi\n",
            "Processing 1.2.1 (4).mp4 - frame 10/217 (4.6%)\n",
            "Processing 1.2.1 (4).mp4 - frame 20/217 (9.2%)\n",
            "Processing 1.2.1 (4).mp4 - frame 30/217 (13.8%)\n",
            "Processing 1.2.1 (4).mp4 - frame 40/217 (18.4%)\n",
            "Processing 1.2.1 (4).mp4 - frame 50/217 (23.0%)\n",
            "Processing 1.2.1 (4).mp4 - frame 60/217 (27.6%)\n",
            "Processing 1.2.1 (4).mp4 - frame 70/217 (32.3%)\n",
            "Processing 1.2.1 (4).mp4 - frame 80/217 (36.9%)\n",
            "Processing 1.2.1 (4).mp4 - frame 90/217 (41.5%)\n",
            "Processing 1.2.1 (4).mp4 - frame 100/217 (46.1%)\n",
            "Processing 1.2.1 (4).mp4 - frame 110/217 (50.7%)\n",
            "Processing 1.2.1 (4).mp4 - frame 120/217 (55.3%)\n",
            "Processing 1.2.1 (4).mp4 - frame 130/217 (59.9%)\n",
            "Processing 1.2.1 (4).mp4 - frame 140/217 (64.5%)\n",
            "Processing 1.2.1 (4).mp4 - frame 150/217 (69.1%)\n",
            "Processing 1.2.1 (4).mp4 - frame 160/217 (73.7%)\n",
            "Processing 1.2.1 (4).mp4 - frame 170/217 (78.3%)\n",
            "Processing 1.2.1 (4).mp4 - frame 180/217 (82.9%)\n",
            "Processing 1.2.1 (4).mp4 - frame 190/217 (87.6%)\n",
            "Processing 1.2.1 (4).mp4 - frame 200/217 (92.2%)\n",
            "Processing 1.2.1 (4).mp4 - frame 210/217 (96.8%)\n",
            "Processing complete. Landmarks saved to 1.2.1 (4)_landmarks.json\n",
            "Processing output_hand_detection (3).mp4 - frame 10/217 (4.6%)\n",
            "Processing output_hand_detection (3).mp4 - frame 20/217 (9.2%)\n",
            "Processing output_hand_detection (3).mp4 - frame 30/217 (13.8%)\n",
            "Processing output_hand_detection (3).mp4 - frame 40/217 (18.4%)\n",
            "Processing output_hand_detection (3).mp4 - frame 50/217 (23.0%)\n",
            "Processing output_hand_detection (3).mp4 - frame 60/217 (27.6%)\n",
            "Processing output_hand_detection (3).mp4 - frame 70/217 (32.3%)\n",
            "Processing output_hand_detection (3).mp4 - frame 80/217 (36.9%)\n",
            "Processing output_hand_detection (3).mp4 - frame 90/217 (41.5%)\n",
            "Processing output_hand_detection (3).mp4 - frame 100/217 (46.1%)\n",
            "Processing output_hand_detection (3).mp4 - frame 110/217 (50.7%)\n",
            "Processing output_hand_detection (3).mp4 - frame 120/217 (55.3%)\n",
            "Processing output_hand_detection (3).mp4 - frame 130/217 (59.9%)\n",
            "Processing output_hand_detection (3).mp4 - frame 140/217 (64.5%)\n",
            "Processing output_hand_detection (3).mp4 - frame 150/217 (69.1%)\n",
            "Processing output_hand_detection (3).mp4 - frame 160/217 (73.7%)\n",
            "Processing output_hand_detection (3).mp4 - frame 170/217 (78.3%)\n",
            "Processing output_hand_detection (3).mp4 - frame 180/217 (82.9%)\n",
            "Processing output_hand_detection (3).mp4 - frame 190/217 (87.6%)\n",
            "Processing output_hand_detection (3).mp4 - frame 200/217 (92.2%)\n",
            "Processing output_hand_detection (3).mp4 - frame 210/217 (96.8%)\n",
            "Processing complete. Landmarks saved to output_hand_detection (3)_landmarks.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_99f789b1-d6b1-4fc2-8597-b3e8807cf48d\", \"1.2.1 (4)_landmarks.json\", 748129)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_43badd93-c75e-4c63-9fc2-75629b1ab209\", \"output_hand_detection (3)_landmarks.json\", 442709)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import mediapipe as mp\n",
        "from google.colab import files\n",
        "\n",
        "def process_video_to_json(input_video_path, gesture_label):\n",
        "    \"\"\"\n",
        "    Process a video and extract hand landmarks to a JSON file\n",
        "\n",
        "    Args:\n",
        "        input_video_path: Path to the input video file\n",
        "        gesture_label: Label for the gesture being performed in the video\n",
        "    \"\"\"\n",
        "    # MediaPipe initialization\n",
        "    BaseOptions = mp.tasks.BaseOptions\n",
        "    HandLandmarker = mp.tasks.vision.HandLandmarker\n",
        "    HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
        "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "    # Create output JSON path\n",
        "    output_json_path = os.path.splitext(input_video_path)[0] + \"_landmarks.json\"\n",
        "\n",
        "    # Create a hand landmarker instance with the video mode\n",
        "    options = HandLandmarkerOptions(\n",
        "        base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
        "        running_mode=VisionRunningMode.VIDEO,\n",
        "        num_hands=2)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {input_video_path}\")\n",
        "        return None\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Initialize data structure for JSON\n",
        "    video_landmarks = {\n",
        "        'video_id': os.path.splitext(os.path.basename(input_video_path))[0],\n",
        "        'gesture_label': gesture_label,\n",
        "        'fps': fps,\n",
        "        'total_frames': total_frames,\n",
        "        'frames': []\n",
        "    }\n",
        "\n",
        "    with HandLandmarker.create_from_options(options) as landmarker:\n",
        "        # Initialize timestamp\n",
        "        timestamp = 0\n",
        "        frame_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            success, frame = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "            # Optional: Print progress\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Processing {input_video_path} - frame {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
        "\n",
        "            # Convert to RGB (MediaPipe requirement)\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "\n",
        "            # Process the frame\n",
        "            results = landmarker.detect_for_video(mp_image, timestamp)\n",
        "\n",
        "            # Calculate timestamp in seconds\n",
        "            timestamp_seconds = frame_count / fps\n",
        "\n",
        "            # Frame landmark data\n",
        "            frame_data = {\n",
        "                'frame_number': frame_count,\n",
        "                'timestamp': timestamp_seconds,\n",
        "                'hands': []\n",
        "            }\n",
        "\n",
        "            # Write hand landmarks to JSON\n",
        "            if results.hand_landmarks:\n",
        "                for idx, hand_landmarks in enumerate(results.hand_landmarks):\n",
        "                    # Get hand label (LEFT or RIGHT)\n",
        "                    handedness = results.handedness[idx][0].category_name\n",
        "\n",
        "                    # Prepare hand landmarks\n",
        "                    hand_data = {\n",
        "                        'hand_type': handedness,\n",
        "                        'landmarks': []\n",
        "                    }\n",
        "\n",
        "                    # Process each landmark\n",
        "                    for landmark_idx, landmark in enumerate(hand_landmarks):\n",
        "                        hand_data['landmarks'].append({\n",
        "                            'landmark_id': landmark_idx,\n",
        "                            'x': landmark.x,\n",
        "                            'y': landmark.y,\n",
        "                            'z': landmark.z\n",
        "                        })\n",
        "\n",
        "                    frame_data['hands'].append(hand_data)\n",
        "\n",
        "            # Add frame data if landmarks were detected\n",
        "            if frame_data['hands']:\n",
        "                video_landmarks['frames'].append(frame_data)\n",
        "\n",
        "            timestamp += 1\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as jsonfile:\n",
        "        json.dump(video_landmarks, jsonfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Processing complete. Landmarks saved to {output_json_path}\")\n",
        "    return output_json_path\n",
        "\n",
        "# อัปโหลดวิดีโอหลายๆ ไฟล์\n",
        "uploaded = files.upload()\n",
        "\n",
        "# ให้ผู้ใช้ระบุท่าทางสำหรับทุกวิดีโอ\n",
        "gesture_label = input(\"กรุณาระบุชื่อท่าทาง (เช่น hello, thank_you, เป็นต้น): \")\n",
        "\n",
        "# เก็บ JSON paths\n",
        "json_paths = []\n",
        "\n",
        "# ประมวลผลวิดีโอแต่ละไฟล์\n",
        "for video_path in uploaded.keys():\n",
        "    json_path = process_video_to_json(video_path, gesture_label)\n",
        "    if json_path:\n",
        "        json_paths.append(json_path)\n",
        "\n",
        "# ดาวน์โหลด JSON ทุกไฟล์\n",
        "for json_path in json_paths:\n",
        "    files.download(json_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "n4QL0iI0tHqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "รวบรวมข้อมูลจากทุกไฟล์ JSONในโฟลเดอร์เพื่อเพิ่มความหลากหลายและความครอบคลุมของข้อมูล"
      ],
      "metadata": {
        "id": "RzxvsUVWtQtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SE6_sPCXaX3g",
        "outputId": "775e1b09-2eb3-4186-a640-5862cac9e125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video data keys: dict_keys(['video_id', 'gesture_label', 'fps', 'total_frames', 'frames'])\n",
            "Number of frames: 48\n",
            "First frame hands: [{'hand_type': 'Right', 'landmarks': [{'landmark_id': 0, 'x': 0.3443603515625, 'y': 0.8842741250991821, 'z': -1.2120239034629776e-07}, {'landmark_id': 1, 'x': 0.3644331097602844, 'y': 0.8599238991737366, 'z': -0.011832674033939838}, {'landmark_id': 2, 'x': 0.3963460624217987, 'y': 0.8404296040534973, 'z': -0.024631474167108536}, {'landmark_id': 3, 'x': 0.4157256782054901, 'y': 0.817276656627655, 'z': -0.03736870363354683}, {'landmark_id': 4, 'x': 0.4262501895427704, 'y': 0.7945399284362793, 'z': -0.051433876156806946}, {'landmark_id': 5, 'x': 0.3917095363140106, 'y': 0.8760520219802856, 'z': -0.028685227036476135}, {'landmark_id': 6, 'x': 0.39549970626831055, 'y': 0.8848051428794861, 'z': -0.04091654717922211}, {'landmark_id': 7, 'x': 0.3912796974182129, 'y': 0.8867673873901367, 'z': -0.050275832414627075}, {'landmark_id': 8, 'x': 0.3873821794986725, 'y': 0.8901175260543823, 'z': -0.05685688555240631}, {'landmark_id': 9, 'x': 0.37654221057891846, 'y': 0.8978257775306702, 'z': -0.031112365424633026}, {'landmark_id': 10, 'x': 0.378012478351593, 'y': 0.9083337187767029, 'z': -0.03854791074991226}, {'landmark_id': 11, 'x': 0.37466102838516235, 'y': 0.9164374470710754, 'z': -0.04330039769411087}, {'landmark_id': 12, 'x': 0.36829978227615356, 'y': 0.9262263178825378, 'z': -0.049453482031822205}, {'landmark_id': 13, 'x': 0.3617047667503357, 'y': 0.9162315130233765, 'z': -0.03365695849061012}, {'landmark_id': 14, 'x': 0.3638369143009186, 'y': 0.9308955669403076, 'z': -0.04276951029896736}, {'landmark_id': 15, 'x': 0.3610232174396515, 'y': 0.9408990144729614, 'z': -0.045151643455028534}, {'landmark_id': 16, 'x': 0.35710710287094116, 'y': 0.9519602060317993, 'z': -0.04805188253521919}, {'landmark_id': 17, 'x': 0.3480346202850342, 'y': 0.929940938949585, 'z': -0.03702634945511818}, {'landmark_id': 18, 'x': 0.3505946695804596, 'y': 0.9457376003265381, 'z': -0.042815931141376495}, {'landmark_id': 19, 'x': 0.3502313196659088, 'y': 0.9566890001296997, 'z': -0.04101055487990379}, {'landmark_id': 20, 'x': 0.34839701652526855, 'y': 0.9680348634719849, 'z': -0.04078099876642227}]}]\n",
            "Video data keys: dict_keys(['video_id', 'gesture_label', 'fps', 'total_frames', 'frames'])\n",
            "Number of frames: 71\n",
            "First frame hands: [{'hand_type': 'Left', 'landmarks': [{'landmark_id': 0, 'x': 0.313771516084671, 'y': 0.965537428855896, 'z': 6.655710649283719e-08}, {'landmark_id': 1, 'x': 0.33894914388656616, 'y': 0.9624893665313721, 'z': -0.01324852742254734}, {'landmark_id': 2, 'x': 0.3651532530784607, 'y': 0.9597474932670593, 'z': -0.02734268642961979}, {'landmark_id': 3, 'x': 0.38185915350914, 'y': 0.9462862610816956, 'z': -0.040745045989751816}, {'landmark_id': 4, 'x': 0.3951246738433838, 'y': 0.928407609462738, 'z': -0.05534052103757858}, {'landmark_id': 5, 'x': 0.3457489311695099, 'y': 0.9973787665367126, 'z': -0.02553163468837738}, {'landmark_id': 6, 'x': 0.34966057538986206, 'y': 1.0136758089065552, 'z': -0.03490730747580528}, {'landmark_id': 7, 'x': 0.35407447814941406, 'y': 1.0211808681488037, 'z': -0.0427650548517704}, {'landmark_id': 8, 'x': 0.35813602805137634, 'y': 1.0253467559814453, 'z': -0.04839610680937767}, {'landmark_id': 9, 'x': 0.32744839787483215, 'y': 1.0084896087646484, 'z': -0.02498994581401348}, {'landmark_id': 10, 'x': 0.330170214176178, 'y': 1.0241668224334717, 'z': -0.030123215168714523}, {'landmark_id': 11, 'x': 0.33367758989334106, 'y': 1.0326827764511108, 'z': -0.03432246297597885}, {'landmark_id': 12, 'x': 0.3354494571685791, 'y': 1.0380380153656006, 'z': -0.03895440697669983}, {'landmark_id': 13, 'x': 0.3111916482448578, 'y': 1.0136781930923462, 'z': -0.024743519723415375}, {'landmark_id': 14, 'x': 0.3121773898601532, 'y': 1.0311639308929443, 'z': -0.02734994888305664}, {'landmark_id': 15, 'x': 0.3166219890117645, 'y': 1.0430494546890259, 'z': -0.028940189629793167}, {'landmark_id': 16, 'x': 0.319950670003891, 'y': 1.0535268783569336, 'z': -0.031719405204057693}, {'landmark_id': 17, 'x': 0.2987765669822693, 'y': 1.01531183719635, 'z': -0.025887951254844666}, {'landmark_id': 18, 'x': 0.2969945967197418, 'y': 1.030174732208252, 'z': -0.027802305296063423}, {'landmark_id': 19, 'x': 0.2986973524093628, 'y': 1.0404858589172363, 'z': -0.026696864515542984}, {'landmark_id': 20, 'x': 0.3009161353111267, 'y': 1.0494894981384277, 'z': -0.026672838255763054}]}]\n",
            "Video data keys: dict_keys(['video_id', 'gesture_label', 'fps', 'total_frames', 'frames'])\n",
            "Number of frames: 71\n",
            "First frame hands: [{'hand_type': 'Left', 'landmarks': [{'landmark_id': 0, 'x': 0.313771516084671, 'y': 0.965537428855896, 'z': 6.655710649283719e-08}, {'landmark_id': 1, 'x': 0.33894914388656616, 'y': 0.9624893665313721, 'z': -0.01324852742254734}, {'landmark_id': 2, 'x': 0.3651532530784607, 'y': 0.9597474932670593, 'z': -0.02734268642961979}, {'landmark_id': 3, 'x': 0.38185915350914, 'y': 0.9462862610816956, 'z': -0.040745045989751816}, {'landmark_id': 4, 'x': 0.3951246738433838, 'y': 0.928407609462738, 'z': -0.05534052103757858}, {'landmark_id': 5, 'x': 0.3457489311695099, 'y': 0.9973787665367126, 'z': -0.02553163468837738}, {'landmark_id': 6, 'x': 0.34966057538986206, 'y': 1.0136758089065552, 'z': -0.03490730747580528}, {'landmark_id': 7, 'x': 0.35407447814941406, 'y': 1.0211808681488037, 'z': -0.0427650548517704}, {'landmark_id': 8, 'x': 0.35813602805137634, 'y': 1.0253467559814453, 'z': -0.04839610680937767}, {'landmark_id': 9, 'x': 0.32744839787483215, 'y': 1.0084896087646484, 'z': -0.02498994581401348}, {'landmark_id': 10, 'x': 0.330170214176178, 'y': 1.0241668224334717, 'z': -0.030123215168714523}, {'landmark_id': 11, 'x': 0.33367758989334106, 'y': 1.0326827764511108, 'z': -0.03432246297597885}, {'landmark_id': 12, 'x': 0.3354494571685791, 'y': 1.0380380153656006, 'z': -0.03895440697669983}, {'landmark_id': 13, 'x': 0.3111916482448578, 'y': 1.0136781930923462, 'z': -0.024743519723415375}, {'landmark_id': 14, 'x': 0.3121773898601532, 'y': 1.0311639308929443, 'z': -0.02734994888305664}, {'landmark_id': 15, 'x': 0.3166219890117645, 'y': 1.0430494546890259, 'z': -0.028940189629793167}, {'landmark_id': 16, 'x': 0.319950670003891, 'y': 1.0535268783569336, 'z': -0.031719405204057693}, {'landmark_id': 17, 'x': 0.2987765669822693, 'y': 1.01531183719635, 'z': -0.025887951254844666}, {'landmark_id': 18, 'x': 0.2969945967197418, 'y': 1.030174732208252, 'z': -0.027802305296063423}, {'landmark_id': 19, 'x': 0.2986973524093628, 'y': 1.0404858589172363, 'z': -0.026696864515542984}, {'landmark_id': 20, 'x': 0.3009161353111267, 'y': 1.0494894981384277, 'z': -0.026672838255763054}]}]\n",
            "Video data keys: dict_keys(['video_id', 'gesture_label', 'fps', 'total_frames', 'frames'])\n",
            "Number of frames: 71\n",
            "First frame hands: [{'hand_type': 'Left', 'landmarks': [{'landmark_id': 0, 'x': 0.313771516084671, 'y': 0.965537428855896, 'z': 6.655710649283719e-08}, {'landmark_id': 1, 'x': 0.33894914388656616, 'y': 0.9624893665313721, 'z': -0.01324852742254734}, {'landmark_id': 2, 'x': 0.3651532530784607, 'y': 0.9597474932670593, 'z': -0.02734268642961979}, {'landmark_id': 3, 'x': 0.38185915350914, 'y': 0.9462862610816956, 'z': -0.040745045989751816}, {'landmark_id': 4, 'x': 0.3951246738433838, 'y': 0.928407609462738, 'z': -0.05534052103757858}, {'landmark_id': 5, 'x': 0.3457489311695099, 'y': 0.9973787665367126, 'z': -0.02553163468837738}, {'landmark_id': 6, 'x': 0.34966057538986206, 'y': 1.0136758089065552, 'z': -0.03490730747580528}, {'landmark_id': 7, 'x': 0.35407447814941406, 'y': 1.0211808681488037, 'z': -0.0427650548517704}, {'landmark_id': 8, 'x': 0.35813602805137634, 'y': 1.0253467559814453, 'z': -0.04839610680937767}, {'landmark_id': 9, 'x': 0.32744839787483215, 'y': 1.0084896087646484, 'z': -0.02498994581401348}, {'landmark_id': 10, 'x': 0.330170214176178, 'y': 1.0241668224334717, 'z': -0.030123215168714523}, {'landmark_id': 11, 'x': 0.33367758989334106, 'y': 1.0326827764511108, 'z': -0.03432246297597885}, {'landmark_id': 12, 'x': 0.3354494571685791, 'y': 1.0380380153656006, 'z': -0.03895440697669983}, {'landmark_id': 13, 'x': 0.3111916482448578, 'y': 1.0136781930923462, 'z': -0.024743519723415375}, {'landmark_id': 14, 'x': 0.3121773898601532, 'y': 1.0311639308929443, 'z': -0.02734994888305664}, {'landmark_id': 15, 'x': 0.3166219890117645, 'y': 1.0430494546890259, 'z': -0.028940189629793167}, {'landmark_id': 16, 'x': 0.319950670003891, 'y': 1.0535268783569336, 'z': -0.031719405204057693}, {'landmark_id': 17, 'x': 0.2987765669822693, 'y': 1.01531183719635, 'z': -0.025887951254844666}, {'landmark_id': 18, 'x': 0.2969945967197418, 'y': 1.030174732208252, 'z': -0.027802305296063423}, {'landmark_id': 19, 'x': 0.2986973524093628, 'y': 1.0404858589172363, 'z': -0.026696864515542984}, {'landmark_id': 20, 'x': 0.3009161353111267, 'y': 1.0494894981384277, 'z': -0.026672838255763054}]}]\n",
            "Video data keys: dict_keys(['video_id', 'gesture_label', 'fps', 'total_frames', 'frames'])\n",
            "Number of frames: 71\n",
            "First frame hands: [{'hand_type': 'Left', 'landmarks': [{'landmark_id': 0, 'x': 0.313771516084671, 'y': 0.965537428855896, 'z': 6.655710649283719e-08}, {'landmark_id': 1, 'x': 0.33894914388656616, 'y': 0.9624893665313721, 'z': -0.01324852742254734}, {'landmark_id': 2, 'x': 0.3651532530784607, 'y': 0.9597474932670593, 'z': -0.02734268642961979}, {'landmark_id': 3, 'x': 0.38185915350914, 'y': 0.9462862610816956, 'z': -0.040745045989751816}, {'landmark_id': 4, 'x': 0.3951246738433838, 'y': 0.928407609462738, 'z': -0.05534052103757858}, {'landmark_id': 5, 'x': 0.3457489311695099, 'y': 0.9973787665367126, 'z': -0.02553163468837738}, {'landmark_id': 6, 'x': 0.34966057538986206, 'y': 1.0136758089065552, 'z': -0.03490730747580528}, {'landmark_id': 7, 'x': 0.35407447814941406, 'y': 1.0211808681488037, 'z': -0.0427650548517704}, {'landmark_id': 8, 'x': 0.35813602805137634, 'y': 1.0253467559814453, 'z': -0.04839610680937767}, {'landmark_id': 9, 'x': 0.32744839787483215, 'y': 1.0084896087646484, 'z': -0.02498994581401348}, {'landmark_id': 10, 'x': 0.330170214176178, 'y': 1.0241668224334717, 'z': -0.030123215168714523}, {'landmark_id': 11, 'x': 0.33367758989334106, 'y': 1.0326827764511108, 'z': -0.03432246297597885}, {'landmark_id': 12, 'x': 0.3354494571685791, 'y': 1.0380380153656006, 'z': -0.03895440697669983}, {'landmark_id': 13, 'x': 0.3111916482448578, 'y': 1.0136781930923462, 'z': -0.024743519723415375}, {'landmark_id': 14, 'x': 0.3121773898601532, 'y': 1.0311639308929443, 'z': -0.02734994888305664}, {'landmark_id': 15, 'x': 0.3166219890117645, 'y': 1.0430494546890259, 'z': -0.028940189629793167}, {'landmark_id': 16, 'x': 0.319950670003891, 'y': 1.0535268783569336, 'z': -0.031719405204057693}, {'landmark_id': 17, 'x': 0.2987765669822693, 'y': 1.01531183719635, 'z': -0.025887951254844666}, {'landmark_id': 18, 'x': 0.2969945967197418, 'y': 1.030174732208252, 'z': -0.027802305296063423}, {'landmark_id': 19, 'x': 0.2986973524093628, 'y': 1.0404858589172363, 'z': -0.026696864515542984}, {'landmark_id': 20, 'x': 0.3009161353111267, 'y': 1.0494894981384277, 'z': -0.026672838255763054}]}]\n",
            "Video data keys: dict_keys(['video_id', 'gesture_label', 'fps', 'total_frames', 'frames'])\n",
            "Number of frames: 71\n",
            "First frame hands: [{'hand_type': 'Left', 'landmarks': [{'landmark_id': 0, 'x': 0.313771516084671, 'y': 0.965537428855896, 'z': 6.655710649283719e-08}, {'landmark_id': 1, 'x': 0.33894914388656616, 'y': 0.9624893665313721, 'z': -0.01324852742254734}, {'landmark_id': 2, 'x': 0.3651532530784607, 'y': 0.9597474932670593, 'z': -0.02734268642961979}, {'landmark_id': 3, 'x': 0.38185915350914, 'y': 0.9462862610816956, 'z': -0.040745045989751816}, {'landmark_id': 4, 'x': 0.3951246738433838, 'y': 0.928407609462738, 'z': -0.05534052103757858}, {'landmark_id': 5, 'x': 0.3457489311695099, 'y': 0.9973787665367126, 'z': -0.02553163468837738}, {'landmark_id': 6, 'x': 0.34966057538986206, 'y': 1.0136758089065552, 'z': -0.03490730747580528}, {'landmark_id': 7, 'x': 0.35407447814941406, 'y': 1.0211808681488037, 'z': -0.0427650548517704}, {'landmark_id': 8, 'x': 0.35813602805137634, 'y': 1.0253467559814453, 'z': -0.04839610680937767}, {'landmark_id': 9, 'x': 0.32744839787483215, 'y': 1.0084896087646484, 'z': -0.02498994581401348}, {'landmark_id': 10, 'x': 0.330170214176178, 'y': 1.0241668224334717, 'z': -0.030123215168714523}, {'landmark_id': 11, 'x': 0.33367758989334106, 'y': 1.0326827764511108, 'z': -0.03432246297597885}, {'landmark_id': 12, 'x': 0.3354494571685791, 'y': 1.0380380153656006, 'z': -0.03895440697669983}, {'landmark_id': 13, 'x': 0.3111916482448578, 'y': 1.0136781930923462, 'z': -0.024743519723415375}, {'landmark_id': 14, 'x': 0.3121773898601532, 'y': 1.0311639308929443, 'z': -0.02734994888305664}, {'landmark_id': 15, 'x': 0.3166219890117645, 'y': 1.0430494546890259, 'z': -0.028940189629793167}, {'landmark_id': 16, 'x': 0.319950670003891, 'y': 1.0535268783569336, 'z': -0.031719405204057693}, {'landmark_id': 17, 'x': 0.2987765669822693, 'y': 1.01531183719635, 'z': -0.025887951254844666}, {'landmark_id': 18, 'x': 0.2969945967197418, 'y': 1.030174732208252, 'z': -0.027802305296063423}, {'landmark_id': 19, 'x': 0.2986973524093628, 'y': 1.0404858589172363, 'z': -0.026696864515542984}, {'landmark_id': 20, 'x': 0.3009161353111267, 'y': 1.0494894981384277, 'z': -0.026672838255763054}]}]\n",
            "Processed 6 samples\n",
            "Labels: ['1' '2' '3' '4' '5' '6']\n",
            "X shape: (6, 30, 42, 3)\n",
            "y shape: (6,)\n",
            "X data type: float64\n",
            "First sample landmarks:\n",
            " [[[ 3.44360352e-01  8.84274125e-01 -1.21202390e-07]\n",
            "  [ 3.64433110e-01  8.59923899e-01 -1.18326740e-02]\n",
            "  [ 3.96346062e-01  8.40429604e-01 -2.46314742e-02]\n",
            "  ...\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[ 7.29501367e-01  8.63729835e-01  1.12840510e-07]\n",
            "  [ 6.90021217e-01  8.46877098e-01 -1.17434524e-02]\n",
            "  [ 6.52324378e-01  8.21962655e-01 -1.73570961e-02]\n",
            "  ...\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[ 3.83579999e-01  8.00637722e-01 -1.82753766e-07]\n",
            "  [ 3.86588991e-01  7.51460016e-01 -1.59670447e-03]\n",
            "  [ 4.05757695e-01  7.06058204e-01 -2.82378448e-03]\n",
            "  ...\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 2.55094498e-01  6.40974641e-01  2.95267114e-07]\n",
            "  [ 2.95845807e-01  6.04234457e-01 -2.00469792e-02]\n",
            "  [ 3.18398595e-01  5.64660788e-01 -2.78833471e-02]\n",
            "  ...\n",
            "  [ 9.10462499e-01  5.23058534e-01 -5.44627868e-02]\n",
            "  [ 9.20367599e-01  4.95648563e-01 -6.18618689e-02]\n",
            "  [ 9.27102983e-01  4.71317589e-01 -6.71759248e-02]]\n",
            "\n",
            " [[ 2.60009855e-01  6.40743792e-01  2.59795769e-07]\n",
            "  [ 2.99506515e-01  6.05670393e-01 -1.79596972e-02]\n",
            "  [ 3.20508569e-01  5.64368427e-01 -2.51179654e-02]\n",
            "  ...\n",
            "  [ 9.06240582e-01  5.20821214e-01 -5.05960733e-02]\n",
            "  [ 9.15365696e-01  4.94664669e-01 -5.75650260e-02]\n",
            "  [ 9.22449350e-01  4.71186817e-01 -6.27173781e-02]]\n",
            "\n",
            " [[ 2.60789365e-01  6.42462552e-01  2.64734950e-07]\n",
            "  [ 3.01472038e-01  6.05071306e-01 -1.61170084e-02]\n",
            "  [ 3.23409796e-01  5.64894855e-01 -2.21730992e-02]\n",
            "  ...\n",
            "  [ 9.02085423e-01  5.23621559e-01 -5.11308126e-02]\n",
            "  [ 9.10061419e-01  4.98639643e-01 -5.87712787e-02]\n",
            "  [ 9.15895164e-01  4.75986898e-01 -6.49307817e-02]]]\n",
            "Labels: [5 1 4 2 0 3]\n",
            "Shape of training data: (4, 30, 42, 3)\n",
            "Unique labels: ['1' '2' '3' '4' '5' '6']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pathlib import Path\n",
        "\n",
        "def preprocess_sign_language_data(data_directory):\n",
        "    \"\"\"\n",
        "    Preprocess sign language data from JSON files\n",
        "\n",
        "    Args:\n",
        "        data_directory: Path to directory containing JSON files\n",
        "\n",
        "    Returns:\n",
        "        X: Processed feature data\n",
        "        y: Corresponding labels\n",
        "    \"\"\"\n",
        "    # เก็บข้อมูลทั้งหมด\n",
        "    all_landmarks = []\n",
        "    all_labels = []\n",
        "\n",
        "    # เพิ่มการตรวจสอบก่อนเริ่มทำงาน\n",
        "    if not os.path.exists(data_directory):\n",
        "        print(f\"Error: Directory {data_directory} does not exist!\")\n",
        "        return [], [], None # Return empty lists and None instead of None, None, None\n",
        "\n",
        "   # วนลูปผ่านไฟล์ JSON โดยตรง\n",
        "    for json_file in os.listdir(data_directory):\n",
        "        if json_file.endswith('.json'):\n",
        "            file_path = os.path.join(data_directory, json_file)\n",
        "\n",
        "            try:\n",
        "                # โหลดข้อมูล JSON\n",
        "                with open(file_path, 'r') as f:\n",
        "                    video_data = json.load(f)\n",
        "\n",
        "                # สกัดคุณลักษณะ\n",
        "                processed_landmarks = process_video_landmarks(video_data)\n",
        "\n",
        "                # เพิ่มข้อมูล\n",
        "                all_landmarks.append(processed_landmarks)\n",
        "                # ใช้ชื่อไฟล์เป็นฉลาก\n",
        "                all_labels.append(json_file.split('_')[0])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # ถ้าไม่พบไฟล์ JSON เลย\n",
        "    if not all_landmarks:\n",
        "        print(f\"No JSON files found in directory {data_directory}\")\n",
        "        return [], [], None # Return empty lists and None instead of None, None, None\n",
        "\n",
        "    # แปลง list เป็น numpy array\n",
        "    X = np.array(all_landmarks)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(all_labels)\n",
        "\n",
        "    # พิมพ์ข้อมูลดีบัก\n",
        "    print(f\"Processed {len(X)} samples\")\n",
        "    print(f\"Labels: {label_encoder.classes_}\")\n",
        "\n",
        "    return X, y, label_encoder\n",
        "\n",
        "def process_video_landmarks(video_data):\n",
        "    \"\"\"\n",
        "    แปลงข้อมูลแลนด์มาร์คจาก JSON เป็นชุดข้อมูลที่เหมาะสำหรับโมเดล\n",
        "\n",
        "    Args:\n",
        "        video_data: ข้อมูล JSON ของวิดีโอ\n",
        "\n",
        "    Returns:\n",
        "        processed_landmarks: อาร์เรย์ของแลนด์มาร์ค\n",
        "    \"\"\"\n",
        "    # เลือกเฟรมที่มีการตรวจจับมือ\n",
        "    hand_frames = [frame for frame in video_data['frames'] if frame['hands']]\n",
        "\n",
        "    # เลือกเฟรมทั้งหมด (หรือจำกัดจำนวนเฟรม)\n",
        "    selected_frames = hand_frames[:30]  # จำกัดที่ 30 เฟรม\n",
        "\n",
        "    # เตรียมอาร์เรย์เก็บแลนด์มาร์ค\n",
        "    landmarks_sequence = []\n",
        "\n",
        "    for frame in selected_frames:\n",
        "        # สำหรับแต่ละมือในเฟรม\n",
        "        frame_landmarks = []\n",
        "        for hand in frame['hands']:\n",
        "            # สกัด x, y, z ของแต่ละจุด\n",
        "            hand_landmarks = [\n",
        "                [landmark['x'], landmark['y'], landmark['z']]\n",
        "                for landmark in hand['landmarks']\n",
        "            ]\n",
        "            frame_landmarks.extend(hand_landmarks)\n",
        "\n",
        "        # padding หากมีจุดไม่ครบ\n",
        "        while len(frame_landmarks) < 42:  # 21 จุด * 2 มือ\n",
        "            frame_landmarks.append([0, 0, 0])\n",
        "\n",
        "        landmarks_sequence.append(frame_landmarks[:42])\n",
        "\n",
        "    # padding sequence ให้มีความยาวคงที่\n",
        "    while len(landmarks_sequence) < 30:\n",
        "        landmarks_sequence.append([[0, 0, 0]] * 42)\n",
        "\n",
        "    # เพิ่มในฟังก์ชัน process_video_landmarks เพื่อดีบัก\n",
        "    print(\"Video data keys:\", video_data.keys())\n",
        "    print(\"Number of frames:\", len(video_data['frames']))\n",
        "    print(\"First frame hands:\", video_data['frames'][0]['hands'])\n",
        "\n",
        "    return np.array(landmarks_sequence)\n",
        "\n",
        "\n",
        "# ตัวอย่างการใช้งาน\n",
        "data_directory = '/content/hi'  # ตรวจสอบให้แน่ใจว่าพาธนี้ถูกต้อง\n",
        "# Check if the directory exists\n",
        "if not os.path.exists(data_directory):\n",
        "    print(f\"Error: Directory '{data_directory}' does not exist. Please create it and add your JSON files.\")\n",
        "else:\n",
        "    # Check if the directory contains any JSON files\n",
        "    json_files = [f for f in os.listdir(data_directory) if f.endswith('.json')]\n",
        "    if not json_files:\n",
        "        print(f\"Error: Directory '{data_directory}' does not contain any JSON files. Please add your JSON files.\")\n",
        "    else:\n",
        "        X, y, label_encoder = preprocess_sign_language_data(data_directory)\n",
        "\n",
        "        if len(X) > 0 and len(y) > 0 and label_encoder is not None:\n",
        "\n",
        "          # เพิ่มบรรทัดนี้ก่อนการแบ่งข้อมูล\n",
        "            print(\"X shape:\", X.shape)\n",
        "            print(\"y shape:\", y.shape)\n",
        "            print(\"X data type:\", X.dtype)\n",
        "            print(\"First sample landmarks:\\n\", X[0])\n",
        "            print(\"Labels:\", y)\n",
        "            # แบ่งข้อมูล\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            print(\"Shape of training data:\", X_train.shape)\n",
        "            print(\"Unique labels:\", label_encoder.classes_)\n",
        "        else:\n",
        "            print(\"No data to process. Check your JSON files.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "h2q27gKz1H20"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}