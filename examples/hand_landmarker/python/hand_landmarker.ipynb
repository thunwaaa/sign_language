{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thunwaaa/sign_language/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Hand Landmarks Detection with MediaPipe Tasks\n",
        "\n",
        "This notebook shows you how to use MediaPipe Tasks Python API to detect hand landmarks from images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with installing MediaPipe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf>=5.26.1,<6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyHeT5Wx0LNv",
        "outputId": "e47caec5-61a0-42b8-847d-6954b9030c8d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 6.0: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gxbHBsF-8Y_l",
        "outputId": "e8cbf2e9-02c8-4f39-b096-2e4af9f53e02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe --upgrade"
      ],
      "metadata": {
        "id": "ud9tSu78qK-N",
        "outputId": "e84ad1ac-7362-4ec2-be22-d04636bb50c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOIHSoTM0-f8",
        "outputId": "595cfcd7-215d-405c-ae75-a06bbce32691"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.0 in /usr/local/lib/python3.11/dist-packages (1.26.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Then download an off-the-shelf model bundle. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker#models) for more information about this model bundle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYKAJ5nDU8-I"
      },
      "source": [
        "## Visualization utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "s3E6NFV-00Qt"
      },
      "outputs": [],
      "source": [
        "#@markdown We implemented some functions to visualize the hand landmark detection results. <br/> Run the following cell to activate the functions.\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions\n",
        "\n",
        "# Setup MediaPipe solutions\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "# Constants\n",
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # vibrant green\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    hand_landmarks_list = detection_result.multi_hand_landmarks\n",
        "    handedness_list = detection_result.multi_handedness\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    # Loop through the detected hands to visualize.\n",
        "    for idx in range(len(hand_landmarks_list)):\n",
        "        hand_landmarks = hand_landmarks_list[idx]\n",
        "        handedness = handedness_list[idx]\n",
        "\n",
        "        # Draw the hand landmarks directly using MediaPipe's drawing utilities\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "        # Get the top left corner of the detected hand's bounding box.\n",
        "        height, width, _ = annotated_image.shape\n",
        "        x_coordinates = [landmark.x for landmark in hand_landmarks.landmark]\n",
        "        y_coordinates = [landmark.y for landmark in hand_landmarks.landmark]\n",
        "        text_x = int(min(x_coordinates) * width)\n",
        "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "        # Draw handedness (left or right hand) on the image.\n",
        "        cv2.putText(annotated_image, f\"{handedness.classification[0].label}\",\n",
        "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Download test image\n",
        "\n",
        "Let's grab a test image that we'll use later. The image is from [Unsplash](https://unsplash.com/photos/mt2fyrdXxzk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tzXuqyIBlXer",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-skLwMBmMN_"
      },
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "etBjSdwImQPw",
        "outputId": "8d9927df-68b6-41be-b5f2-49cd33003a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fd406a62-b0c9-45aa-a699-4105946e1b59\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fd406a62-b0c9-45aa-a699-4105946e1b59\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1.2.1.mp4 to 1.2.1 (1).mp4\n",
            "Uploaded: 1.2.1 (1).mp4\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # อัปโหลดวิดีโอของคุณ\n",
        "\n",
        "# เก็บชื่อไฟล์ที่อัปโหลด\n",
        "input_video_path = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded: {input_video_path}\")\n",
        "\n",
        "# กำหนดชื่อไฟล์ CSV output\n",
        "output_json_path = input_video_path.split('.')[0] + \"_landmarks.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "\n",
        "Here are the steps to run hand landmark detection using MediaPipe.\n",
        "\n",
        "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/python) to learn more about configuration options that this solution supports.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# import urllib.request\n",
        "\n",
        "# # MediaPipe initialization\n",
        "# BaseOptions = mp.tasks.BaseOptions\n",
        "# HandLandmarker = mp.tasks.vision.HandLandmarker\n",
        "# HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
        "# VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "# # Create a hand landmarker instance with the video mode\n",
        "# options = HandLandmarkerOptions(\n",
        "#     base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
        "#     running_mode=VisionRunningMode.VIDEO,\n",
        "#     num_hands=2)\n",
        "\n",
        "# # Open the video file\n",
        "# cap = cv2.VideoCapture(input_video_path)\n",
        "# if not cap.isOpened():\n",
        "#     print(f\"Error: Could not open video file {input_video_path}\")\n",
        "#     exit()\n",
        "\n",
        "# # Get video properties\n",
        "# width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "# total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# # Create video writer for output\n",
        "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "# out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# with HandLandmarker.create_from_options(options) as landmarker:\n",
        "#     # Initialize timestamp\n",
        "#     timestamp = 0\n",
        "#     frame_count = 0\n",
        "\n",
        "#     while cap.isOpened():\n",
        "#         success, frame = cap.read()\n",
        "#         if not success:\n",
        "#             print(\"End of video or error reading frame.\")\n",
        "#             break\n",
        "\n",
        "#         frame_count += 1\n",
        "#         # Optional: Print progress\n",
        "#         if frame_count % 10 == 0:\n",
        "#             print(f\"Processing frame {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
        "\n",
        "#         # Convert to RGB (MediaPipe requirement)\n",
        "#         frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "#         mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "\n",
        "#         # Process the frame\n",
        "#         results = landmarker.detect_for_video(mp_image, timestamp)\n",
        "#         timestamp += 1\n",
        "\n",
        "#         # Create a black canvas instead of using the original frame\n",
        "#         canvas = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "\n",
        "#         # Draw hand landmarks\n",
        "#         if results.hand_landmarks:\n",
        "#             for idx, hand_landmarks in enumerate(results.hand_landmarks):\n",
        "#                 # Get hand label (LEFT or RIGHT)\n",
        "#                 handedness = results.handedness[idx][0].category_name\n",
        "\n",
        "#                 # Get center of hand for text placement\n",
        "#                 x_values = [landmark.x for landmark in hand_landmarks]\n",
        "#                 y_values = [landmark.y for landmark in hand_landmarks]\n",
        "#                 center_x = int(sum(x_values) / len(x_values) * width)\n",
        "#                 center_y = int(sum(y_values) / len(y_values) * height)\n",
        "\n",
        "#                 # Draw connections with white color\n",
        "#                 for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
        "#                     start_idx = connection[0]\n",
        "#                     end_idx = connection[1]\n",
        "\n",
        "#                     start_point = (int(hand_landmarks[start_idx].x * width),\n",
        "#                                   int(hand_landmarks[start_idx].y * height))\n",
        "#                     end_point = (int(hand_landmarks[end_idx].x * width),\n",
        "#                                 int(hand_landmarks[end_idx].y * height))\n",
        "\n",
        "#                     cv2.line(canvas, start_point, end_point, (255, 255, 255), 2)  # White lines\n",
        "\n",
        "#                 # Draw landmarks with light blue color\n",
        "#                 for landmark in hand_landmarks:\n",
        "#                     landmark_point = (int(landmark.x * width),\n",
        "#                                      int(landmark.y * height))\n",
        "#                     cv2.circle(canvas, landmark_point, 5, (255, 200, 0), -1)  # Light blue dots\n",
        "\n",
        "#                 # Display hand label\n",
        "#                 color = (255, 100, 100) if handedness == \"LEFT\" else (100, 100, 255)  # Different colors for left/right\n",
        "#                 text_position = (center_x, center_y - 30)\n",
        "#                 cv2.putText(canvas, handedness, text_position,\n",
        "#                            cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "\n",
        "#         # Write the canvas to output video\n",
        "#         out.write(canvas)\n",
        "\n",
        "#         # Optional: Display the frame (comment out for faster processing)\n",
        "#         # cv2.imshow('Processing Video', canvas)\n",
        "#         # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         #     break\n",
        "\n",
        "# # Release resources\n",
        "# cap.release()\n",
        "# out.release()\n",
        "# cv2.destroyAllWindows()\n",
        "\n",
        "# print(f\"Processing complete. Output saved to {output_video_path}\")\n",
        "\n",
        "# files.download(output_video_path)"
      ],
      "metadata": {
        "id": "0HLgVzd8A3qT",
        "cellView": "form"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_JVO3rvPD4RN",
        "outputId": "de93d0bf-a1da-4bf5-83e9-f22beb868de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "กรุณาระบุชื่อท่าทาง (เช่น hello, thank_you, เป็นต้น): hi\n",
            "Processing frame 10/217 (4.6%)\n",
            "Processing frame 20/217 (9.2%)\n",
            "Processing frame 30/217 (13.8%)\n",
            "Processing frame 40/217 (18.4%)\n",
            "Processing frame 50/217 (23.0%)\n",
            "Processing frame 60/217 (27.6%)\n",
            "Processing frame 70/217 (32.3%)\n",
            "Processing frame 80/217 (36.9%)\n",
            "Processing frame 90/217 (41.5%)\n",
            "Processing frame 100/217 (46.1%)\n",
            "Processing frame 110/217 (50.7%)\n",
            "Processing frame 120/217 (55.3%)\n",
            "Processing frame 130/217 (59.9%)\n",
            "Processing frame 140/217 (64.5%)\n",
            "Processing frame 150/217 (69.1%)\n",
            "Processing frame 160/217 (73.7%)\n",
            "Processing frame 170/217 (78.3%)\n",
            "Processing frame 180/217 (82.9%)\n",
            "Processing frame 190/217 (87.6%)\n",
            "Processing frame 200/217 (92.2%)\n",
            "Processing frame 210/217 (96.8%)\n",
            "Processing complete. Landmarks saved to 1_landmarks.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9e5fe8a-f38f-4919-86e0-57742ac9c5b3\", \"1_landmarks.json\", 748129)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import mediapipe as mp\n",
        "\n",
        "def process_video_to_json(input_video_path, output_json_path, gesture_label):\n",
        "    \"\"\"\n",
        "    Process a video and extract hand landmarks to a JSON file\n",
        "\n",
        "    Args:\n",
        "        input_video_path: Path to the input video file\n",
        "        output_json_path: Path to save the output JSON file\n",
        "        gesture_label: Label for the gesture being performed in the video\n",
        "    \"\"\"\n",
        "    # MediaPipe initialization\n",
        "    BaseOptions = mp.tasks.BaseOptions\n",
        "    HandLandmarker = mp.tasks.vision.HandLandmarker\n",
        "    HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
        "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "    # Create a hand landmarker instance with the video mode\n",
        "    options = HandLandmarkerOptions(\n",
        "        base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
        "        running_mode=VisionRunningMode.VIDEO,\n",
        "        num_hands=2)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {input_video_path}\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Initialize data structure for JSON\n",
        "    video_landmarks = {\n",
        "        'video_id': os.path.splitext(os.path.basename(input_video_path))[0],\n",
        "        'gesture_label': gesture_label,\n",
        "        'fps': fps,\n",
        "        'total_frames': total_frames,\n",
        "        'frames': []\n",
        "    }\n",
        "\n",
        "    with HandLandmarker.create_from_options(options) as landmarker:\n",
        "        # Initialize timestamp\n",
        "        timestamp = 0\n",
        "        frame_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            success, frame = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "            # Optional: Print progress\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Processing frame {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
        "\n",
        "            # Convert to RGB (MediaPipe requirement)\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "\n",
        "            # Process the frame\n",
        "            results = landmarker.detect_for_video(mp_image, timestamp)\n",
        "\n",
        "            # Calculate timestamp in seconds\n",
        "            timestamp_seconds = frame_count / fps\n",
        "\n",
        "            # Frame landmark data\n",
        "            frame_data = {\n",
        "                'frame_number': frame_count,\n",
        "                'timestamp': timestamp_seconds,\n",
        "                'hands': []\n",
        "            }\n",
        "\n",
        "            # Write hand landmarks to JSON\n",
        "            if results.hand_landmarks:\n",
        "                for idx, hand_landmarks in enumerate(results.hand_landmarks):\n",
        "                    # Get hand label (LEFT or RIGHT)\n",
        "                    handedness = results.handedness[idx][0].category_name\n",
        "\n",
        "                    # Prepare hand landmarks\n",
        "                    hand_data = {\n",
        "                        'hand_type': handedness,\n",
        "                        'landmarks': []\n",
        "                    }\n",
        "\n",
        "                    # Process each landmark\n",
        "                    for landmark_idx, landmark in enumerate(hand_landmarks):\n",
        "                        hand_data['landmarks'].append({\n",
        "                            'landmark_id': landmark_idx,\n",
        "                            'x': landmark.x,\n",
        "                            'y': landmark.y,\n",
        "                            'z': landmark.z\n",
        "                        })\n",
        "\n",
        "                    frame_data['hands'].append(hand_data)\n",
        "\n",
        "            # Add frame data if landmarks were detected\n",
        "            if frame_data['hands']:\n",
        "                video_landmarks['frames'].append(frame_data)\n",
        "\n",
        "            timestamp += 1\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as jsonfile:\n",
        "        json.dump(video_landmarks, jsonfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Processing complete. Landmarks saved to {output_json_path}\")\n",
        "\n",
        "# รับชื่อท่าทางจากผู้ใช้\n",
        "gesture_label = input(\"กรุณาระบุชื่อท่าทาง (เช่น hello, thank_you, เป็นต้น): \")\n",
        "\n",
        "# ประมวลผลวิดีโอ\n",
        "process_video_to_json(input_video_path, output_json_path, gesture_label)\n",
        "\n",
        "# ดาวน์โหลดไฟล์ JSON\n",
        "files.download(output_json_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "n4QL0iI0tHqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "รวบรวมข้อมูลจากทุกไฟล์ JSONในโฟลเดอร์เพื่อเพิ่มความหลากหลายและความครอบคลุมของข้อมูล"
      ],
      "metadata": {
        "id": "RzxvsUVWtQtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SE6_sPCXaX3g",
        "outputId": "b201c886-7cda-4813-d6e5-0d47c66e4951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking if path exists: False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\sitta\\\\Documents\\\\project\\\\hi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-ea4b6a940fd1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checking if path exists:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sign_language_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# แบ่งข้อมูล\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-ea4b6a940fd1>\u001b[0m in \u001b[0;36mpreprocess_sign_language_data\u001b[0;34m(data_directory)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# วนลูปผ่านโฟลเดอร์คำศัพท์\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mgesture_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgesture_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\sitta\\\\Documents\\\\project\\\\hi'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def preprocess_sign_language_data(data_directory):\n",
        "    \"\"\"\n",
        "    Preprocess sign language data from JSON files\n",
        "\n",
        "    Args:\n",
        "        data_directory: Path to directory containing JSON files\n",
        "\n",
        "    Returns:\n",
        "        X: Processed feature data\n",
        "        y: Corresponding labels\n",
        "    \"\"\"\n",
        "    # เก็บข้อมูลทั้งหมด\n",
        "    all_landmarks = []\n",
        "    all_labels = []\n",
        "\n",
        "    # วนลูปผ่านโฟลเดอร์คำศัพท์\n",
        "    for gesture_folder in os.listdir(data_directory):\n",
        "        folder_path = os.path.join(data_directory, gesture_folder)\n",
        "\n",
        "        # ข้ามถ้าไม่ใช่โฟลเดอร์\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        # วนลูปผ่านไฟล์ JSON\n",
        "        for json_file in os.listdir(folder_path):\n",
        "            if json_file.endswith('.json'):\n",
        "                file_path = os.path.join(folder_path, json_file)\n",
        "\n",
        "                # โหลดข้อมูล JSON\n",
        "                with open(file_path, 'r') as f:\n",
        "                    video_data = json.load(f)\n",
        "\n",
        "                # สกัดคุณลักษณะ\n",
        "                processed_landmarks = process_video_landmarks(video_data)\n",
        "\n",
        "                # เพิ่มข้อมูล\n",
        "                all_landmarks.append(processed_landmarks)\n",
        "                all_labels.append(gesture_folder)\n",
        "\n",
        "    # แปลง list เป็น numpy array\n",
        "    X = np.array(all_landmarks)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(all_labels)\n",
        "\n",
        "    return X, y, label_encoder\n",
        "\n",
        "def process_video_landmarks(video_data):\n",
        "    \"\"\"\n",
        "    แปลงข้อมูลแลนด์มาร์คจาก JSON เป็นชุดข้อมูลที่เหมาะสำหรับโมเดล\n",
        "\n",
        "    Args:\n",
        "        video_data: ข้อมูล JSON ของวิดีโอ\n",
        "\n",
        "    Returns:\n",
        "        processed_landmarks: อาร์เรย์ของแลนด์มาร์ค\n",
        "    \"\"\"\n",
        "    # เลือกเฟรมที่มีการตรวจจับมือ\n",
        "    hand_frames = [frame for frame in video_data['frames'] if frame['hands']]\n",
        "\n",
        "    # เลือกเฟรมทั้งหมด (หรือจำกัดจำนวนเฟรม)\n",
        "    selected_frames = hand_frames[:30]  # จำกัดที่ 30 เฟรม\n",
        "\n",
        "    # เตรียมอาร์เรย์เก็บแลนด์มาร์ค\n",
        "    landmarks_sequence = []\n",
        "\n",
        "    for frame in selected_frames:\n",
        "        # สำหรับแต่ละมือในเฟรม\n",
        "        frame_landmarks = []\n",
        "        for hand in frame['hands']:\n",
        "            # สกัด x, y, z ของแต่ละจุด\n",
        "            hand_landmarks = [\n",
        "                [landmark['x'], landmark['y'], landmark['z']]\n",
        "                for landmark in hand['landmarks']\n",
        "            ]\n",
        "            frame_landmarks.extend(hand_landmarks)\n",
        "\n",
        "        # padding หากมีจุดไม่ครบ\n",
        "        while len(frame_landmarks) < 42:  # 21 จุด * 2 มือ\n",
        "            frame_landmarks.append([0, 0, 0])\n",
        "\n",
        "        landmarks_sequence.append(frame_landmarks[:42])\n",
        "\n",
        "    # padding sequence ให้มีความยาวคงที่\n",
        "    while len(landmarks_sequence) < 30:\n",
        "        landmarks_sequence.append([[0, 0, 0]] * 42)\n",
        "\n",
        "    return np.array(landmarks_sequence)\n",
        "\n",
        "# ตัวอย่างการใช้งาน\n",
        "data_directory = r\"C:\\Users\\sitta\\Documents\\project\\hi\"\n",
        "print(\"Checking if path exists:\", os.path.exists(data_directory))\n",
        "\n",
        "X, y, label_encoder = preprocess_sign_language_data(data_directory)\n",
        "\n",
        "# แบ่งข้อมูล\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Shape of training data:\", X_train.shape)\n",
        "print(\"Unique labels:\", label_encoder.classes_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "h2q27gKz1H20"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}